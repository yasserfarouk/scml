from typing import Any

import gymnasium as gym
from gymnasium.envs.registration import register

from scml.std.agent import StdAgent
from scml.std.agents import StdDummyAgent
from scml.std.rl.action import ActionManager
from scml.std.rl.factory import FixedPartnerNumbersStdFactory, StdWorldFactory
from scml.std.rl.observation import ObservationManager
from scml.std.rl.reward import DefaultRewardFunction, RewardFunction
from scml.std.world import SCML2024StdWorld

__all__ = ["StdEnv"]


class StdEnv(gym.Env):
    def __init__(
        self,
        action_manager: ActionManager,
        observation_manager: ObservationManager,
        reward_function: RewardFunction = DefaultRewardFunction(),
        render_mode=None,
        factory: StdWorldFactory = FixedPartnerNumbersStdFactory(),
        agent_type: type[StdAgent] = StdDummyAgent,
        agent_params: dict[str, Any] | None = None,
        extra_checks: bool = True,
        skip_after_negotiations: bool = True,
    ):
        assert action_manager.factory in factory, (
            f"Action Manager is not compatible with the given environment.\n"
            f"Some worlds that can be generated by this environment are not handled"
            f" correctly by this action manager"
        )

        assert observation_manager.factory in factory, (
            f"observation Manager is not compatible with the given environment.\n"
            f"Some worlds that can be generated by this environment are not handled"
            f" correctly by this observation manager"
        )
        self._skip_after_negotiations = skip_after_negotiations
        self._extra_checks = extra_checks
        self._reward_function = reward_function
        self._world: SCML2024StdWorld = None  # type: ignore
        self._agent_type = agent_type
        self._agent_params = agent_params if agent_params is not None else dict()
        self._agent_id: str = ""
        self._agent: StdAgent = None  # type: ignore
        self._obs_manager = observation_manager
        self._action_manager = action_manager
        self._factory = factory
        self.action_space = action_manager.make_space()
        self.observation_space = observation_manager.make_space()
        self.render_mode = render_mode
        # self.reset()

    def _get_obs(self):
        return self._obs_manager.encode(self._agent.awi.state)
        # return {"agent": self._agent_location, "target": self._target_location}

    def _get_info(self):
        return dict()
        # return {
        #     "distance": np.linalg.norm(
        #         self._agent_location - self._target_location, ord=1
        #     )
        # }

    def _render_frame(self):
        pass

    def close(self):
        pass

    def render(self):
        pass

    def reset(
        self, *, seed: int | None = None, options: dict[str, Any] | None = None
    ) -> tuple[Any, dict[str, Any]]:
        _ = options
        import random

        random.seed(seed)
        self._world, agents = self._factory(
            types=(self._agent_type,),
            params=(self._agent_params,),
        )
        assert len(agents) == 1
        self._agent = agents[0]
        if self._extra_checks:
            assert self._world in self._factory
        self._agent_id = self._agent.id
        self._world.step_with(dict(), init=True)
        observation = self._get_obs()
        info = self._get_info()

        if self.render_mode == "human":
            self._render_frame()

        return observation, info

    def step(self, action):
        reward_info = self._reward_function.before_action(self._agent.awi)
        # score_before = self._world.scores()[self._agent_id]
        decoded_action = self._action_manager.decode(self._agent.awi, action)
        terminated = not self._world.step_with(
            {self._agent_id: decoded_action}  # type: ignore
        )
        reward = self._reward_function(self._agent.awi, decoded_action, reward_info)
        if self._world.current_step >= self._world.n_steps - 1:
            terminated = 1
        if self._skip_after_negotiations:
            while not terminated and len(self._agent.awi.current_states) < 1:
                reward_info = self._reward_function.before_action(self._agent.awi)
                terminated = not self._world.step_with(
                    {self._agent_id: dict()}  # type: ignore
                )
                reward += self._reward_function(self._agent.awi, dict(), reward_info)
                if self._world.current_step >= self._world.n_steps - 1:
                    terminated = 1
        observation = self._get_obs()
        info = self._get_info()

        if self.render_mode == "human":
            self._render_frame()

        return observation, reward, terminated, False, info


register(
    id="scml/Std-v0",
    entry_point="scml.std.rl.env:StdEnv",
    max_episode_steps=None,
)
